import os
import sys
import pandas as pd
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from tqdm.asyncio import tqdm_asyncio

# ================= 0. å¯¼å…¥ Google GenAI SDK =================
try:
    from google import genai
    from google.genai import types
    print("æˆåŠŸå¯¼å…¥ google.genai")
except ImportError:
    print("ã€é”™è¯¯ã€‘éœ€è¦å®‰è£… google-genai åº“ (pip install google-genai)")
    exit(1)

# ================= 1. å¯¼å…¥æœ¬åœ°è¯„æµ‹æ¨¡å— =================
JUDGE_PATH = "/data/home/the/rxliu/projects/open-r1-main/tests/utils"
if JUDGE_PATH not in sys.path:
    sys.path.append(JUDGE_PATH)

try:
    from llm_judge import llm_judge_via_api
    print("æˆåŠŸå¯¼å…¥ llm_judge_via_api")
except ImportError:
    print(f"ã€é”™è¯¯ã€‘æ— æ³•ä» {JUDGE_PATH} å¯¼å…¥ llm_judge_via_api")
    exit(1)

# ================= 2. é…ç½®åŒºåŸŸ =================
INPUT_FILE = "/ssd5/rxliu/datasets/SFT-Data/DeepScaleR/test.parquet"
OUTPUT_BASE = INPUT_FILE.replace(".parquet", "_gemini-3-pro-preview_results")

# --- ç”Ÿæˆæ¨¡å‹é…ç½® (Google Gemini) ---
# è¯·å¡«å…¥ä½ çš„ Google/Vertex API Key
GEN_API_KEY = "sk-wz6J5hsrKSCAuvE5eRJ2Q70OQHMNJxl3KMLC2ANSVdJIbv13" 
GEN_BASE_URL_PROXY = "https://api.openai-proxy.org/google"
GEN_MODEL_NAME = "gemini-3-pro-preview" # æˆ– gemini-2.0-flash-thinking-exp

# --- è¯„æµ‹æ¨¡å‹é…ç½® (Qwen/DashScope) ---
# ã€æ³¨æ„ã€‘è¿™é‡Œå¿…é¡»å¡«å…¥ DashScope çš„ API Keyï¼Œå› ä¸ºè¯„æµ‹ç”¨çš„æ˜¯ Qwen
JUDGE_API_KEY = "sk-YOUR_DASHSCOPE_KEY_HERE" 
JUDGE_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
JUDGE_MODEL_NAME = "qwen3-next-80b-a3b-instruct"

# --- æ€§èƒ½å‚æ•° ---
MAX_ATTEMPTS = 2 
MAX_CONCURRENCY = 100  # Gemini çš„å¹¶å‘é™åˆ¶é€šå¸¸æ¯” Qwen ä¸¥ï¼Œå»ºè®®å…ˆè®¾ 100ï¼Œå¦‚ç¨³å®šå¯è°ƒé«˜
REQUEST_TIMEOUT = 1200.0

judge_executor = ThreadPoolExecutor(max_workers=32)

def extract_last_boxed_content(text):
    """æå– \\boxed{...}"""
    if not text: return None
    idx = text.rfind("\\boxed{")
    if idx == -1: return None 

    content_start = idx + 7 
    balance = 0
    content_end = -1
    
    for i in range(content_start, len(text)):
        char = text[i]
        if char == '{':
            balance += 1
        elif char == '}':
            if balance == 0:
                content_end = i
                break
            balance -= 1
            
    if content_end != -1:
        return text[content_start:content_end]
    return None

def run_judge_sync(predicted, ground_truth):
    """åŒæ­¥è¯„æµ‹å‡½æ•°"""
    try:
        if not predicted: return False
        is_correct = llm_judge_via_api(
            predicted, 
            ground_truth, 
            JUDGE_API_URL, 
            JUDGE_API_KEY, 
            JUDGE_MODEL_NAME
        )
        return is_correct
    except Exception as e:
        return False

async def get_gemini_response_async(client, prompt):
    """
    ä½¿ç”¨ Google GenAI SDK è·å–å›å¤ï¼Œè§£ææ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ
    """
    try:
        # é…ç½® Thinking
        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_level="High",
                include_thoughts=True
            ),
            response_mime_type="text/plain"
        )

        # å¼‚æ­¥è°ƒç”¨ (.aio)
        response = await client.aio.models.generate_content(
            model=GEN_MODEL_NAME,
            contents=prompt,
            config=config
        )

        reasoning_parts = []
        answer_parts = []
        
        # è§£æ Parts (åˆ†ç¦» thought å’Œ text)
        if response.parts:
            for part in response.parts:
                if part.thought:
                    # æŸäº› SDK ç‰ˆæœ¬ text å­—æ®µåœ¨ thought ä¸º True æ—¶å­˜æ”¾æ€è€ƒå†…å®¹
                    reasoning_parts.append(part.text or "")
                else:
                    answer_parts.append(part.text or "")
        
        full_reasoning = "\n".join(reasoning_parts).strip()
        full_answer = "\n".join(answer_parts).strip()
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ thought part (æœ‰æ—¶å€™æ¨¡å‹å¯èƒ½ä¸è§¦å‘ thinking)ï¼Œåšä¸ªå…œåº•
        if not full_answer and response.text:
            full_answer = response.text

        # æå– Token ä½¿ç”¨é‡
        thoughts_tokens = 0
        prompt_tokens = 0
        
        if response.usage_metadata:
            thoughts_tokens = response.usage_metadata.thoughts_token_count or 0
            prompt_tokens = response.usage_metadata.prompt_token_count or 0

        return full_reasoning, full_answer, thoughts_tokens, prompt_tokens, None

    except Exception as e:
        err_str = str(e)
        # ç®€å•åˆ¤æ–­ Rate Limit (Google çš„é”™è¯¯ç é€šå¸¸åœ¨ message é‡Œ)
        if "429" in err_str or "ResourceExhausted" in err_str:
            return "", "", 0, 0, "RATE_LIMIT"
        return "", "", 0, 0, f"Error: {err_str}"

async def process_single_problem(sem, client, idx, row):
    async with sem:
        problem_text = row['problem']
        ground_truth = row['answer']
        problem_results = []

        for attempt in range(1, MAX_ATTEMPTS + 1):
            retry_wait = 2
            
            # --- API ç”Ÿæˆ ---
            while True:
                reasoning, answer, t_tokens, p_tokens, error = await get_gemini_response_async(client, problem_text)
                
                if error == "RATE_LIMIT":
                    await asyncio.sleep(retry_wait)
                    retry_wait = min(retry_wait * 2, 60)
                    continue
                elif error:
                    reasoning = f"[API Error] {error}"
                    break # å…¶ä»–é”™è¯¯è·³å‡ºé‡è¯•
                else:
                    break 

            # --- åˆ¤é¢˜å‡†å¤‡ ---
            judge_input = None
            extracted_boxed = None
            judge_type = "fail"
            
            if not error:
                extracted_boxed = extract_last_boxed_content(answer)
                if extracted_boxed:
                    judge_input = extracted_boxed
                    judge_type = "boxed"
                else:
                    judge_input = answer 
                    judge_type = "full_text"
            
            # --- æ‰§è¡Œåˆ¤é¢˜ ---
            if judge_input:
                loop = asyncio.get_running_loop()
                is_correct = await loop.run_in_executor(
                    judge_executor, 
                    run_judge_sync, 
                    judge_input,
                    ground_truth
                )
            else:
                is_correct = False

            record = {
                "id": idx,
                "problem": problem_text,
                "ground_truth": ground_truth,
                "attempt": attempt,
                "gen_reasoning": reasoning, # Gemini æ€è€ƒè¿‡ç¨‹
                "gen_answer": answer,       # Gemini æœ€ç»ˆå›ç­”
                "tokens_thinking": t_tokens, # æ€è€ƒ Token æ•°
                "tokens_prompt": p_tokens,   # æç¤ºè¯ Token æ•°
                "extracted_boxed": extracted_boxed, 
                "judge_input_type": judge_type,
                "is_correct": is_correct
            }
            problem_results.append(record)

            if is_correct:
                break
        
        return problem_results

async def main():
    # åˆå§‹åŒ– Google GenAI Client
    # æ³¨æ„ï¼šgenai.Client è‡ªå·±ç®¡ç†è¿æ¥æ± ï¼Œé€šå¸¸ä¸éœ€è¦ä¼ å…¥ httpx client
    client = genai.Client(
        api_key=GEN_API_KEY,
        vertexai=True,
        http_options={
            "base_url": GEN_BASE_URL_PROXY,
            "api_version": "v1alpha" # é¢„è§ˆç‰ˆåŠŸèƒ½é€šå¸¸éœ€è¦ alpha ç‰ˆæœ¬
        },
    )

    print(f"è¯»å–æ–‡ä»¶: {INPUT_FILE}...")
    try:
        df = pd.read_parquet(INPUT_FILE)
        print(f"æˆåŠŸåŠ è½½ï¼Œå…± {len(df)} æ¡æ•°æ®ã€‚")
    except Exception as e:
        print(f"è¯»å–å¤±è´¥: {e}")
        return

    sem = asyncio.Semaphore(MAX_CONCURRENCY)
    
    print("="*60)
    print(f"ğŸš€ å…¨é€Ÿå¯åŠ¨ | å¹¶å‘: {MAX_CONCURRENCY} | Model: {GEN_MODEL_NAME}")
    print(f"æ¨¡å¼: Gemini Thinking Mode (High) + æœ¬åœ° Judge")
    print("="*60)

    tasks = [process_single_problem(sem, client, idx, row) for idx, row in df.iterrows()]
    
    start_time = time.time()
    results_nested = await tqdm_asyncio.gather(*tasks)
    all_results = [item for sublist in results_nested for item in sublist]
    elapsed = time.time() - start_time

    if not all_results: return
    
    # ä¿å­˜ç»“æœ
    df_res = pd.DataFrame(all_results).sort_values(by=['id', 'attempt'])
    
    print(f"æ­£åœ¨ä¿å­˜ ({len(df_res)}æ¡è®°å½•)...")
    df_res.to_excel(OUTPUT_BASE + "_all.xlsx", index=False)
    
    df_correct = df_res[df_res['is_correct'] == True]
    df_correct.to_excel(OUTPUT_BASE + "_correct.xlsx", index=False)
    
    # æœ€ç»ˆç»Ÿè®¡
    uniq_correct = len(df_correct['id'].unique())
    total_thinking_tokens = df_res['tokens_thinking'].sum()
    
    print("-" * 30)
    print(f"è€—æ—¶: {elapsed:.1f}s | åå: {len(df)/elapsed:.2f} TPS")
    print(f"å‡†ç¡®ç‡: {uniq_correct}/{len(df)} ({uniq_correct/len(df):.2%})")
    print(f"æ€»æ€è€ƒ Tokens: {total_thinking_tokens} | å¹³å‡: {total_thinking_tokens/len(df_res):.1f}")

    judge_executor.shutdown()

if __name__ == "__main__":
    try:
        import uvloop
        uvloop.install()
    except: pass
    asyncio.run(main())