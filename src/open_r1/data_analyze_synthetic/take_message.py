import pandas as pd
import json
from datasets import Dataset
import os

# ================= é…ç½® =================
TEST_FILE = "/ssd5/rxliu/datasets/SFT-Data/DeepScaleR/generate_test/test_qwen3-max_graph_results_correct.xlsx"
OUTPUT_DIR = "/ssd5/rxliu/datasets/SFT-Data/DeepScaleR/sft_format"

os.makedirs(OUTPUT_DIR, exist_ok=True)

def create_conversation(row):
    """
    ä»æ•°æ®è¡Œåˆ›å»ºå¯¹è¯æ ¼å¼
    è¾“å…¥: problem
    è¾“å‡º: graph_structured_reasoning (åŒ…å« <think> å’Œ <answer>)
    """
    conversation = [
        {
            "role": "user",
            "content": f"Question: {row['problem']}"
        },
        {
            "role": "assistant",
            "content": row['graph_structured_reasoning']
        }
    ]
    return conversation

def main():
    print("="*80)
    print("è½¬æ¢ Test æ•°æ®é›†ä¸º SFT æ ¼å¼")
    print("="*80)
    
    # ========== 1. è¯»å–æ–‡ä»¶ ==========
    print(f"\nğŸ“– è¯»å–æ–‡ä»¶: {TEST_FILE}")
    
    if not os.path.exists(TEST_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {TEST_FILE}")
        return
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
    if TEST_FILE.endswith('.xlsx'):
        df_test = pd.read_excel(TEST_FILE)
        print(f"âœ“ è¯»å– Excel æ–‡ä»¶ï¼Œå…± {len(df_test)} æ¡è®°å½•")
    elif TEST_FILE.endswith('.parquet'):
        df_test = pd.read_parquet(TEST_FILE)
        print(f"âœ“ è¯»å– Parquet æ–‡ä»¶ï¼Œå…± {len(df_test)} æ¡è®°å½•")
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {TEST_FILE}")
        return
    
    # æ˜¾ç¤ºåˆ—å
    print(f"âœ“ æ•°æ®åˆ—: {df_test.columns.tolist()}")
    
    # æ£€æŸ¥å¿…éœ€çš„åˆ—
    required_cols = ['problem', 'graph_structured_reasoning']
    missing_cols = [col for col in required_cols if col not in df_test.columns]
    if missing_cols:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
        return
    
    # ========== 2. è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼ ==========
    print(f"\nğŸ”„ è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼...")
    
    test_conversations = []
    for idx, row in df_test.iterrows():
        conversation = create_conversation(row)
        test_conversations.append({
            "messages": conversation,
            "source": "test_correct",
            "problem_id": row['id'] if 'id' in row else idx
        })
    
    print(f"âœ“ æˆåŠŸè½¬æ¢ {len(test_conversations)} æ¡æ•°æ®")
    
    # ========== 3. ä¿å­˜ä¸ºè®­ç»ƒæ ¼å¼ï¼ˆåªæœ‰ messagesï¼‰ ==========
    print(f"\nğŸ’¾ ä¿å­˜è®­ç»ƒæ ¼å¼ï¼ˆåªæœ‰ messages å­—æ®µï¼‰...")
    
    # åªä¿ç•™ messages å­—æ®µ
    test_data_clean = [{"messages": item["messages"]} for item in test_conversations]
    
    # JSONL æ ¼å¼
    test_jsonl = os.path.join(OUTPUT_DIR, "test.jsonl")
    with open(test_jsonl, 'w', encoding='utf-8') as f:
        for item in test_data_clean:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ“ JSONL: {test_jsonl}")
    
    # Parquet æ ¼å¼
    test_parquet = os.path.join(OUTPUT_DIR, "test.parquet")
    pd.DataFrame(test_data_clean).to_parquet(test_parquet, index=False)
    print(f"âœ“ Parquet: {test_parquet}")
    
    # HuggingFace Dataset æ ¼å¼
    test_dataset = Dataset.from_pandas(pd.DataFrame(test_data_clean))
    test_dataset_dir = os.path.join(OUTPUT_DIR, "test_dataset")
    test_dataset.save_to_disk(test_dataset_dir)
    print(f"âœ“ HF Dataset: {test_dataset_dir}")
    
    # ========== 4. ä¿å­˜å®Œæ•´ç‰ˆæœ¬ï¼ˆåŒ…å«é¢å¤–å­—æ®µï¼‰ ==========
    print(f"\nğŸ’¾ ä¿å­˜å®Œæ•´ç‰ˆæœ¬ï¼ˆåŒ…å« source å’Œ problem_idï¼‰...")
    
    test_full_parquet = os.path.join(OUTPUT_DIR, "test_full.parquet")
    pd.DataFrame(test_conversations).to_parquet(test_full_parquet, index=False)
    print(f"âœ“ å®Œæ•´ç‰ˆ Parquet: {test_full_parquet}")
    
    # ========== 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ ==========
    print(f"\n{'='*80}")
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    print(f"{'='*80}")
    print(f"æ€»æ•°æ®é‡: {len(test_conversations)} æ¡")
    
    # æ£€æŸ¥æ•°æ®å†…å®¹é•¿åº¦
    content_lengths = [len(item['messages'][1]['content']) for item in test_conversations]
    print(f"\næ¨¡å‹è¾“å‡ºé•¿åº¦ç»Ÿè®¡:")
    print(f"  - æœ€çŸ­: {min(content_lengths)} å­—ç¬¦")
    print(f"  - æœ€é•¿: {max(content_lengths)} å­—ç¬¦")
    print(f"  - å¹³å‡: {sum(content_lengths)/len(content_lengths):.0f} å­—ç¬¦")
    
    # ========== 6. æ˜¾ç¤ºç¤ºä¾‹ ==========
    print(f"\n{'='*80}")
    print("ğŸ“ æ•°æ®ç¤ºä¾‹ï¼ˆå‰2æ¡ï¼‰:")
    print(f"{'='*80}")
    
    for i, item in enumerate(test_data_clean[:2], 1):
        print(f"\nã€ç¤ºä¾‹ {i}ã€‘")
        print("-"*80)
        for msg in item['messages']:
            print(f"\n{msg['role'].upper()}:")
            content = msg['content']
            if len(content) > 300:
                print(content[:300] + "...")
            else:
                print(content)
        print("="*80)
    
    # ========== 7. ä½¿ç”¨è¯´æ˜ ==========
    print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"ã€ç”¨äºè®­ç»ƒ/è¯„ä¼°ã€‘ï¼ˆåªæœ‰ messages å­—æ®µï¼‰")
    print(f"  - test.jsonl          â† æ¨èä½¿ç”¨")
    print(f"  - test.parquet")
    print(f"  - test_dataset/")
    print(f"\nã€ç”¨äºåˆ†æã€‘ï¼ˆåŒ…å«é¢å¤–å­—æ®µï¼‰")
    print(f"  - test_full.parquet")
    print(f"\nä¿å­˜ä½ç½®: {OUTPUT_DIR}")
    print(f"\nä½¿ç”¨æ–¹å¼:")
    print(f"python sft.py \\")
    print(f"    --dataset_name {OUTPUT_DIR}/train_mixed_shuffled.jsonl \\")
    print(f"    --eval_dataset_name {OUTPUT_DIR}/test.jsonl \\")
    print(f"    --eval_strategy steps \\")
    print(f"    --eval_steps 500 \\")
    print(f"    ...")

if __name__ == "__main__":
    main()