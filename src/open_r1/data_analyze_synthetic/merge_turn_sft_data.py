import pandas as pd
import json
from datasets import Dataset
import os
import random

# ================= é…ç½® =================
BASE_DIR = "/ssd5/rxliu/datasets/SFT-Data/DeepScaleR/split_files"
TEST_FILE = "/ssd5/rxliu/datasets/SFT-Data/DeepScaleR/generate_test/test_qwen3-max_graph_results_correct.xlsx"
OUTPUT_DIR = "/ssd5/rxliu/datasets/SFT-Data/DeepScaleR/sft_format"

# 4ä¸ªæ–‡ä»¶çš„è·¯å¾„
FILE_PARTS = [
    "train_part_1_of_4",
    "train_part_2_of_4",
    "train_part_3_of_4",
    "train_part_4_of_4",
]

# éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
RANDOM_SEED = 42

os.makedirs(OUTPUT_DIR, exist_ok=True)

def create_conversation_from_correct(row):
    """
    ä» correct æ•°æ®åˆ›å»ºå¯¹è¯æ ¼å¼
    ä½¿ç”¨ graph_structured_reasoningï¼ˆåŒ…å« <think> å’Œ <answer>ï¼‰
    """
    conversation = [
        {
            "role": "user",
            "content": f"Question: {row['problem']}"
        },
        {
            "role": "assistant",
            "content": row['graph_structured_reasoning']
        }
    ]
    return conversation

def create_conversation_from_unsolved(row):
    """
    ä» unsolved æ•°æ®åˆ›å»ºå¯¹è¯æ ¼å¼
    ä½¿ç”¨ solution + answerï¼ˆæ ‡å‡†ç­”æ¡ˆæ ¼å¼ï¼‰
    """
    # æ„å»ºæ ‡å‡†ç­”æ¡ˆæ ¼å¼
    if pd.notna(row['ground_truth_solution']) and row['ground_truth_solution'].strip():
        # æœ‰è¯¦ç»†è§£é¢˜æ­¥éª¤
        assistant_content = f"<think>\n{row['ground_truth_solution']}\n</think>\n<answer>\n{row['ground_truth_answer']}\n</answer>"
    else:
        # åªæœ‰ç­”æ¡ˆï¼Œæ²¡æœ‰è¯¦ç»†æ­¥éª¤
        assistant_content = f"<answer>\n{row['ground_truth_answer']}\n</answer>"
    
    conversation = [
        {
            "role": "user",
            "content": f"Question: {row['problem']}"
        },
        {
            "role": "assistant",
            "content": assistant_content
        }
    ]
    return conversation

def main():
    print("="*80)
    print("å¼€å§‹è½¬æ¢ SFT è®­ç»ƒæ•°æ®ï¼ˆå¤„ç†4ä¸ªæ–‡ä»¶ + testæ•°æ®ï¼‰")
    print("="*80)
    
    all_correct_conversations = []
    all_unsolved_conversations = []
    
    # ========== 1. å¾ªç¯å¤„ç†4ä¸ªéƒ¨åˆ†çš„æ–‡ä»¶ï¼ˆæ¯éƒ¨åˆ†æœ‰ correct å’Œ unsolvedï¼‰ ==========
    for part_name in FILE_PARTS:
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ å¤„ç†: {part_name}")
        print(f"{'='*80}")
        
        correct_file = os.path.join(BASE_DIR, f"{part_name}_qwen3-max_graph_results_correct.parquet")
        unsolved_file = os.path.join(BASE_DIR, f"{part_name}_qwen3-max_graph_results_unsolved.parquet")
        
        # --- è¯»å– correct æ•°æ® ---
        print(f"\nğŸ“– è¯»å– correct æ–‡ä»¶: {os.path.basename(correct_file)}")
        if os.path.exists(correct_file):
            df_correct = pd.read_parquet(correct_file)
            print(f"âœ“ è¯»å– {len(df_correct)} æ¡ correct è®°å½•")
            
            # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
            for idx, row in df_correct.iterrows():
                conversation = create_conversation_from_correct(row)
                all_correct_conversations.append({
                    "messages": conversation,
                    "source": "correct",
                    "part": part_name,
                    "problem_id": row['id']
                })
            print(f"âœ“ æœ¬æ–‡ä»¶è½¬æ¢ {len(df_correct)} æ¡ï¼Œç´¯è®¡ correct æ•°æ®: {len(all_correct_conversations)} æ¡")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        # --- è¯»å– unsolved æ•°æ® ---
        print(f"\nğŸ“– è¯»å– unsolved æ–‡ä»¶: {os.path.basename(unsolved_file)}")
        if os.path.exists(unsolved_file):
            df_unsolved = pd.read_parquet(unsolved_file)
            print(f"âœ“ è¯»å– {len(df_unsolved)} æ¡ unsolved è®°å½•")
            
            # ç»Ÿè®¡ solution ä¸ºç©ºçš„æ•°é‡
            empty_solution = df_unsolved['ground_truth_solution'].isna() | (df_unsolved['ground_truth_solution'].str.strip() == '')
            empty_count = empty_solution.sum()
            print(f"  - å…¶ä¸­ {empty_count} æ¡æ²¡æœ‰è¯¦ç»† solution")
            print(f"  - {len(df_unsolved) - empty_count} æ¡æœ‰è¯¦ç»† solution")
            
            # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
            for idx, row in df_unsolved.iterrows():
                conversation = create_conversation_from_unsolved(row)
                all_unsolved_conversations.append({
                    "messages": conversation,
                    "source": "unsolved",
                    "part": part_name,
                    "problem_id": row['problem_id']
                })
            print(f"âœ“ æœ¬æ–‡ä»¶è½¬æ¢ {len(df_unsolved)} æ¡ï¼Œç´¯è®¡ unsolved æ•°æ®: {len(all_unsolved_conversations)} æ¡")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
    
    # ========== 2. ç»Ÿè®¡æ€»æ•° ==========
    print(f"\n{'='*80}")
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    print(f"{'='*80}")
    print(f"âœ“ Correct æ•°æ®æ€»è®¡: {len(all_correct_conversations)} æ¡")
    print(f"âœ“ Unsolved æ•°æ®æ€»è®¡: {len(all_unsolved_conversations)} æ¡")
    print(f"âœ“ æ€»æ•°æ®é‡: {len(all_correct_conversations) + len(all_unsolved_conversations)} æ¡")
    
    # ========== 3. ä¿å­˜åˆ†å¼€çš„æ–‡ä»¶ ==========
    print(f"\n{'='*80}")
    print("ğŸ’¾ ä¿å­˜åˆ†å¼€çš„æ–‡ä»¶ï¼ˆcorrect å’Œ unsolvedï¼‰")
    print(f"{'='*80}")
    
    # ä¿å­˜ correct æ•°æ®
    if all_correct_conversations:
        # JSONL
        correct_jsonl = os.path.join(OUTPUT_DIR, "train_correct_only.jsonl")
        with open(correct_jsonl, 'w', encoding='utf-8') as f:
            for item in all_correct_conversations:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"âœ“ Correct JSONL: {correct_jsonl}")
        
        # Parquet
        correct_parquet = os.path.join(OUTPUT_DIR, "train_correct_only.parquet")
        pd.DataFrame(all_correct_conversations).to_parquet(correct_parquet, index=False)
        print(f"âœ“ Correct Parquet: {correct_parquet}")
    
    # ä¿å­˜ unsolved æ•°æ®
    if all_unsolved_conversations:
        # JSONL
        unsolved_jsonl = os.path.join(OUTPUT_DIR, "train_unsolved_only.jsonl")
        with open(unsolved_jsonl, 'w', encoding='utf-8') as f:
            for item in all_unsolved_conversations:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"âœ“ Unsolved JSONL: {unsolved_jsonl}")
        
        # Parquet
        unsolved_parquet = os.path.join(OUTPUT_DIR, "train_unsolved_only.parquet")
        pd.DataFrame(all_unsolved_conversations).to_parquet(unsolved_parquet, index=False)
        print(f"âœ“ Unsolved Parquet: {unsolved_parquet}")
    
    # ========== 4. æ··åˆå¹¶æ‰“æ•£æ•°æ® ==========
    print(f"\n{'='*80}")
    print("ğŸ”€ æ··åˆå¹¶æ‰“æ•£æ•°æ®")
    print(f"{'='*80}")
    
    all_conversations = all_correct_conversations + all_unsolved_conversations
    
    # è®¾ç½®éšæœºç§å­å¹¶æ‰“æ•£
    random.seed(RANDOM_SEED)
    random.shuffle(all_conversations)
    print(f"âœ“ ä½¿ç”¨éšæœºç§å­ {RANDOM_SEED} æ‰“æ•£æ•°æ®")
    print(f"âœ“ æ‰“æ•£åæ€»è®¡: {len(all_conversations)} æ¡")
    
    # ========== 5. ä¿å­˜æ··åˆåçš„æ•°æ® ==========
    print(f"\n{'='*80}")
    print("ğŸ’¾ ä¿å­˜æ··åˆæ•°æ®ï¼ˆå·²æ‰“æ•£ï¼‰")
    print(f"{'='*80}")
    
    # åªä¿ç•™ messages å­—æ®µç”¨äºè®­ç»ƒ
    training_data = [{"messages": item["messages"]} for item in all_conversations]
    
    # JSONL æ ¼å¼ï¼ˆè®­ç»ƒç”¨ï¼Œåªæœ‰ messagesï¼‰
    mixed_jsonl = os.path.join(OUTPUT_DIR, "train_mixed_shuffled.jsonl")
    with open(mixed_jsonl, 'w', encoding='utf-8') as f:
        for item in training_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ“ æ··åˆ JSONL (è®­ç»ƒç”¨): {mixed_jsonl}")
    
    # Parquet æ ¼å¼ï¼ˆè®­ç»ƒç”¨ï¼Œåªæœ‰ messagesï¼‰
    mixed_parquet = os.path.join(OUTPUT_DIR, "train_mixed_shuffled.parquet")
    pd.DataFrame(training_data).to_parquet(mixed_parquet, index=False)
    print(f"âœ“ æ··åˆ Parquet (è®­ç»ƒç”¨): {mixed_parquet}")
    
    # HuggingFace Dataset æ ¼å¼ï¼ˆè®­ç»ƒç”¨ï¼Œåªæœ‰ messagesï¼‰
    dataset = Dataset.from_pandas(pd.DataFrame(training_data))
    dataset_dir = os.path.join(OUTPUT_DIR, "train_mixed_shuffled_dataset")
    dataset.save_to_disk(dataset_dir)
    print(f"âœ“ HF Dataset (è®­ç»ƒç”¨): {dataset_dir}")
    
    # ä¿å­˜å®Œæ•´ç‰ˆæœ¬ï¼ˆåŒ…å«é¢å¤–å­—æ®µï¼Œç”¨äºåˆ†æï¼‰
    mixed_full_parquet = os.path.join(OUTPUT_DIR, "train_mixed_shuffled_full.parquet")
    pd.DataFrame(all_conversations).to_parquet(mixed_full_parquet, index=False)
    print(f"âœ“ å®Œæ•´ç‰ˆ Parquet (åŒ…å«source/part/id): {mixed_full_parquet}")
    
    # ========== 6. æœ€ç»ˆç»Ÿè®¡ ==========
    print(f"\n{'='*80}")
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print(f"{'='*80}")
    
    # ç»Ÿè®¡å„éƒ¨åˆ†æ¥æº
    from collections import Counter
    part_counter = Counter([item['part'] for item in all_conversations])
    print("\nå„æ–‡ä»¶è´¡çŒ®æ•°æ®é‡:")
    for part, count in sorted(part_counter.items()):
        print(f"  - {part}: {count} æ¡")
    
    # ç»Ÿè®¡ correct/unsolved å æ¯”
    source_counter = Counter([item['source'] for item in all_conversations])
    print("\næ•°æ®æ¥æºåˆ†å¸ƒ:")
    for source, count in source_counter.items():
        percentage = count / len(all_conversations) * 100
        print(f"  - {source}: {count} æ¡ ({percentage:.1f}%)")
    
    # ========== 7. æ˜¾ç¤ºç¤ºä¾‹ ==========
    print(f"\n{'='*80}")
    print("ğŸ“ æ‰“æ•£åçš„æ•°æ®ç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:")
    print(f"{'='*80}")
    
    for i, item in enumerate(all_conversations[:3], 1):
        print(f"\nã€ç¤ºä¾‹ {i}ã€‘")
        print(f"æ¥æº: {item['source']} | æ–‡ä»¶: {item['part']} | ID: {item['problem_id']}")
        print("-"*80)
        for msg in item['messages']:
            print(f"\n{msg['role'].upper()}:")
            content = msg['content']
            if len(content) > 200:
                print(content[:200] + "...")
            else:
                print(content)
        print("="*80)
    
    print("\nâœ… è½¬æ¢å®Œæˆï¼")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"ã€è®­ç»ƒé›†ã€‘")
    print(f"  1. åˆ†å¼€ä¿å­˜:")
    print(f"     - Correct: train_correct_only.jsonl / .parquet")
    print(f"     - Unsolved: train_unsolved_only.jsonl / .parquet")
    print(f"  2. æ··åˆæ‰“æ•£:")
    print(f"     - æ¨èä½¿ç”¨: train_mixed_shuffled.jsonl")
    print(f"     - æˆ–: train_mixed_shuffled.parquet")
    print(f"     - æˆ–: train_mixed_shuffled_dataset/")
    print(f"\nã€æµ‹è¯•é›†ã€‘")
    print(f"     - test.jsonl")
    print(f"     - test.parquet")
    print(f"     - test_dataset/")
    print(f"\nä¿å­˜ä½ç½®: {OUTPUT_DIR}")

    # ========== é¢å¤–å¤„ç†ï¼šè½¬æ¢ test æ•°æ®é›† ==========
    print(f"\n{'='*80}")
    print("ğŸ“‚ å¤„ç† Test æ•°æ®é›†")
    print(f"{'='*80}")
    
    if os.path.exists(TEST_FILE):
        print(f"ğŸ“– è¯»å–: {os.path.basename(TEST_FILE)}")
        
        # è¯»å– Excel æˆ– Parquetï¼ˆæ ¹æ®æ‰©å±•ååˆ¤æ–­ï¼‰
        if TEST_FILE.endswith('.xlsx'):
            df_test = pd.read_excel(TEST_FILE)
        elif TEST_FILE.endswith('.parquet'):
            df_test = pd.read_parquet(TEST_FILE)
        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè·³è¿‡")
            return
        
        print(f"âœ“ è¯»å– {len(df_test)} æ¡ test è®°å½•")
        
        # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
        test_conversations = []
        for idx, row in df_test.iterrows():
            conversation = create_conversation_from_correct(row)
            test_conversations.append({
                "messages": conversation,
                "source": "test_correct",
                "problem_id": row['id'] if 'id' in row else idx
            })
        
        print(f"âœ“ è½¬æ¢ {len(test_conversations)} æ¡ test æ•°æ®")
        
        # ä¿å­˜ test æ•°æ®ï¼ˆä¸æ‰“æ•£ï¼Œä¿æŒåŸé¡ºåºï¼‰
        print(f"\nğŸ’¾ ä¿å­˜ test æ•°æ®...")
        
        # JSONL
        test_jsonl = os.path.join(OUTPUT_DIR, "test.jsonl")
        with open(test_jsonl, 'w', encoding='utf-8') as f:
            for item in test_conversations:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"âœ“ Test JSONL: {test_jsonl}")
        
        # Parquet
        test_parquet = os.path.join(OUTPUT_DIR, "test.parquet")
        pd.DataFrame(test_conversations).to_parquet(test_parquet, index=False)
        print(f"âœ“ Test Parquet: {test_parquet}")
        
        # HF Dataset
        test_dataset = Dataset.from_pandas(pd.DataFrame(test_conversations))
        test_dataset_dir = os.path.join(OUTPUT_DIR, "test_dataset")
        test_dataset.save_to_disk(test_dataset_dir)
        print(f"âœ“ Test HF Dataset: {test_dataset_dir}")
        
        print(f"\nğŸ“Š Test æ•°æ®ç»Ÿè®¡: {len(test_conversations)} æ¡")
    else:
        print(f"âš ï¸  Test æ–‡ä»¶ä¸å­˜åœ¨: {TEST_FILE}")
        print("   è·³è¿‡ test æ•°æ®è½¬æ¢")

if __name__ == "__main__":
    main()