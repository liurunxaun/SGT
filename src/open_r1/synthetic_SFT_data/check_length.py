import json
import argparse
import numpy as np
from transformers import AutoTokenizer
from tqdm import tqdm
import os

def calculate_token_lengths(data_path, model_path):
    print(f"Loading tokenizer from: {model_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return

    print(f"Loading data from: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        # å¤„ç†å¯èƒ½çš„ JSONL æ ¼å¼æˆ–æ ‡å‡† JSON åˆ—è¡¨æ ¼å¼
        try:
            data = json.load(f)
            if not isinstance(data, list):
                print("Error: JSON content is not a list.")
                return
        except json.JSONDecodeError:
            # å°è¯•è¯»å– JSONL
            f.seek(0)
            data = [json.loads(line) for line in f]

    print(f"Total samples: {len(data)}")
    
    lengths = []
    
    print("Calculating token lengths...")
    for item in tqdm(data):
        text_content = ""
        
        # è‡ªåŠ¨è¯†åˆ«å¸¸è§æ ¼å¼
        # 1. å¸¸è§çš„ ShareGPT/ChatML æ ¼å¼ (conversations / messages)
        if "conversations" in item:
            msgs = item["conversations"]
            # ä½¿ç”¨ tokenizer çš„æ¨¡ç‰ˆå¤„ç†ï¼Œè¿™æ ·æœ€å‡†ï¼ˆåŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼‰
            if hasattr(tokenizer, "apply_chat_template"):
                try:
                    # æˆ‘ä»¬åªç®— token é•¿åº¦ï¼Œä¸éœ€è¦ç”Ÿæˆ
                    encoded = tokenizer.apply_chat_template(msgs, tokenize=True)
                    lengths.append(len(encoded))
                    continue
                except:
                    pass # å¦‚æœæ¨¡ç‰ˆå¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬æ‹¼æ¥
            
            # æ‰‹åŠ¨æ‹¼æ¥ fallback
            for msg in msgs:
                text_content += str(msg.get("value", "")) or str(msg.get("content", ""))

        elif "messages" in item:
            msgs = item["messages"]
            if hasattr(tokenizer, "apply_chat_template"):
                try:
                    encoded = tokenizer.apply_chat_template(msgs, tokenize=True)
                    lengths.append(len(encoded))
                    continue
                except:
                    pass
            for msg in msgs:
                text_content += str(msg.get("content", ""))

        # 2. Alpaca æ ¼å¼ (instruction / input / output)
        elif "instruction" in item and "output" in item:
            text_content = item["instruction"] + "\n" + item.get("input", "") + "\n" + item["output"]
            
        # 3. çº¯æ–‡æœ¬æ ¼å¼ (text)
        elif "text" in item:
            text_content = item["text"]
            
        else:
            # å®åœ¨ä¸çŸ¥é“å•¥æ ¼å¼ï¼Œå°±æŠŠæ‰€æœ‰ value æ‹¼èµ·æ¥ç®—ä¸ªå¤§æ¦‚
            text_content = " ".join([str(v) for v in item.values()])

        # Tokenize
        token_ids = tokenizer.encode(text_content, add_special_tokens=True)
        lengths.append(len(token_ids))

    if not lengths:
        print("No valid data found.")
        return

    # ç»Ÿè®¡åˆ†æ
    lengths = np.array(lengths)
    max_len = np.max(lengths)
    min_len = np.min(lengths)
    avg_len = np.mean(lengths)
    p50 = np.percentile(lengths, 50)
    p90 = np.percentile(lengths, 90)
    p95 = np.percentile(lengths, 95)
    p98 = np.percentile(lengths, 98)
    p99 = np.percentile(lengths, 99)

    print("\n" + "="*40)
    print("ğŸ“Š æ•°æ®é•¿åº¦ç»Ÿè®¡æŠ¥å‘Š (Tokens)")
    print("="*40)
    print(f"æ•°æ®æ€»é‡: {len(lengths)}")
    print(f"æœ€å°é•¿åº¦: {int(min_len)}")
    print(f"å¹³å‡é•¿åº¦: {int(avg_len)}")
    print(f"æœ€å¤§é•¿åº¦: {int(max_len)}  <-- åªæœ‰è¿™ä¸ªæ•°å¾ˆå¤§å—ï¼Ÿ")
    print("-" * 20)
    print(f"P50 (ä¸­ä½æ•°): {int(p50)}")
    print(f"P90 (æ¶µç›–90%): {int(p90)}")
    print(f"P95 (æ¶µç›–95%): {int(p95)}")
    print(f"P98 (æ¶µç›–98%): {int(p98)}")
    print(f"P99 (æ¶µç›–99%): {int(p99)}")
    print("="*40)

    # ç»™å‡ºå»ºè®®
    recommended = int(p98)
    # å‘ä¸Šå–æ•´åˆ°æœ€è¿‘çš„ 256 å€æ•° (æ˜¾å¡å‹å¥½)
    recommended = ((recommended // 256) + 1) * 256
    
    # è®¾ç½®ä¸€ä¸ªä¸‹é™ï¼Œæ¯”å¦‚ä¸å°‘äº 2048
    if recommended < 2048: recommended = 2048
    
    print(f"ğŸ’¡ å»ºè®®è®¾ç½® max_seq_length: {recommended}")
    
    if max_len > recommended * 2:
        print(f"âš ï¸ æ³¨æ„: ä½ æœ€å¤§çš„æ•°æ® ({int(max_len)}) è¿œè¶…å»ºè®®å€¼ã€‚")
        print(f"   å¦‚æœä½ è®¾ç½® {recommended}ï¼Œå°†ä¼šæœ‰çº¦ 2% çš„è¶…é•¿æ•°æ®è¢«æˆªæ–­ã€‚")
        print("   å¦‚æœè¿™éƒ¨åˆ†è¶…é•¿æ•°æ®å¾ˆé‡è¦ï¼Œè¯·è€ƒè™‘è®¾å¤§ä¸€ç‚¹ï¼›å¦‚æœåªæ˜¯å™ªéŸ³ï¼Œç›´æ¥æˆªæ–­å³å¯æé€Ÿã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help="Path to json file")
    parser.add_argument("--model_path", type=str, required=True, help="Path to HF model")
    args = parser.parse_args()

    calculate_token_lengths(args.data_path, args.model_path)