import os
import sys
import json
import re
import subprocess
import traceback
import pandas as pd

# ================= 0. ç¯å¢ƒè®¾ç½® =================
sys.path.append("/data/home/the/rxliu/projects/open-r1-main/tests/utils")
from inference_sglang import inference_sglang

# ================= 1. å‚æ•°é…ç½® =================
MODEL_NAME = "Qwen3-8B-Base"
# æ”¹ä¸ªåå­—ï¼Œé¿å…å’Œæ—§æ–‡ä»¶æ··æ·†
TIME_TAG = "20251204-MbppPlus-PromptTrigger" 

SERVER_PORT = 30000
DATASET_PATH = "/ssd5/rxliu/datasets/mbppplus/data/test-00000-of-00001-d5781c9c51e02795.parquet"
DATASET_NAME = "MbppPlus"

# æŒ‰ä½ çš„è¦æ±‚ä¿æŒ 32768
MAX_TOKENS = 32768 
# Base æ¨¡å‹å»ºè®®ä½æ¸©ï¼Œ0.0 æˆ– 0.2
TEMPERATURE = 0.0  

BASE_OUTPUT_DIR = "/ssd5/rxliu/projects/open-r1-main/results"
os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

INFERENCE_OUTPUT = f"{BASE_OUTPUT_DIR}/inference-{MODEL_NAME}-{DATASET_NAME}-{TIME_TAG}.xlsx"
SAMPLES_JSONL = f"{BASE_OUTPUT_DIR}/samples-{MODEL_NAME}-{DATASET_NAME}-{TIME_TAG}.jsonl"

# ================= 3. ä»£ç æ¸…æ´—å‡½æ•° =================
def sanitize_code(text: str) -> str:
    """
    é’ˆå¯¹ Base æ¨¡å‹ + å¼ºåˆ¶ Prompt çš„æ¸…æ´—ç­–ç•¥ã€‚
    å› ä¸º Prompt ç»“å°¾å·²ç»æ˜¯ ```pythonï¼Œæ¨¡å‹è¾“å‡ºçš„é€šå¸¸ç›´æ¥æ˜¯ä»£ç ã€‚
    æˆ‘ä»¬ä¸»è¦è´Ÿè´£æ¸…ç†ç»“å°¾çš„ markdown é—­åˆç¬¦å’Œå¯èƒ½çš„åºŸè¯ã€‚
    """
    if not isinstance(text, str):
        return ""
    
    text = text.strip()
    
    # 1. å»æ‰ç»“å°¾çš„ ``` (ä¸ç®¡æ˜¯ä¸æ˜¯ python)
    text = re.sub(r"```.*$", "", text, flags=re.DOTALL).strip()
    
    # 2. å¦‚æœæ¨¡å‹è¿˜æ˜¯è¾“å‡ºäº† <answer> æ ‡ç­¾ (Qwen ç³»åˆ—ç‰¹æ€§)ï¼Œæå–å†…éƒ¨
    pattern_answer = r"<answer>\s*(.*?)\s*</answer>"
    match = re.search(pattern_answer, text, re.DOTALL | re.IGNORECASE)
    if match:
        text = match.group(1).strip()
        
    return text

# ================= 4. ä¸»ç¨‹åº =================
def main():
    print(f"=== ä»»åŠ¡: {MODEL_NAME} (Base Mode with Prompt Trigger) ===")
    print(f"=== Port: {SERVER_PORT} | Max Tokens: {MAX_TOKENS} ===")
    
    # ---------------- Step 0: é¢„å¤„ç† Prompt (æ ¸å¿ƒä¿®å¤) ----------------
    print(f"\n>>> [0/3] æ„å»º Base æ¨¡å‹ä¸“ç”¨ Prompt...")
    try:
        df = pd.read_parquet(DATASET_PATH)
        
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘
        # ç»™æ¯ä¸ª prompt åé¢å¼ºè¡Œæ‹¼æ¥ "\n```python\n"
        # è¿™æ ·æ¨¡å‹ä¼šä»¥ä¸ºå®ƒæ­£åœ¨è¡¥å…¨ä¸€ä¸ª Markdown ä»£ç å—ï¼Œä»è€Œç›´æ¥è¾“å‡ºä»£ç 
        df["engineered_prompt"] = df["prompt"].apply(
            lambda x: f'{x}\n\n```python\n'
        )
        
        # ä¿å­˜ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¾› Sglang è¯»å–
        temp_parquet = f"{BASE_OUTPUT_DIR}/temp_input_mbpp_base.parquet"
        df.to_parquet(temp_parquet)
        print(f"âœ… ä¸´æ—¶ Prompt æ–‡ä»¶å·²ç”Ÿæˆ: {temp_parquet}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        return

    # ---------------- Step 1: æ¨ç† ----------------
    print(f"\n>>> [1/3] Sglang æ¨ç†...")
    try:
        inference_sglang(
            temp_parquet,            # ä½¿ç”¨å¤„ç†è¿‡çš„æ•°æ®
            "",                      # System Prompt ç•™ç©º (é  engineered_prompt å¼•å¯¼)
            "engineered_prompt",     # ä½¿ç”¨æˆ‘ä»¬æ„é€ çš„å¸¦ trigger çš„åˆ—
            "code",                  
            INFERENCE_OUTPUT,
            MODEL_NAME,
            TEMPERATURE,
            MAX_TOKENS,
        )
    except Exception as e:
        print(f"âŒ æ¨ç†é”™è¯¯: {e}")
        return

    # ---------------- Step 2: è½¬æ¢ JSONL ----------------
    print(f"\n>>> [2/3] è½¬æ¢ä¸º JSONL...")
    if not os.path.exists(INFERENCE_OUTPUT):
        print("âŒ æ¨ç†æ–‡ä»¶æœªç”Ÿæˆ")
        return

    try:
        df_pred = pd.read_excel(INFERENCE_OUTPUT)
        
        # æ¢å¤ task_id (ä»åŸå§‹ df æ‹¿ï¼Œé˜²æ­¢é¡ºåºé”™ä¹±æˆ–ä¸¢å¤±)
        if len(df_pred) == len(df):
            df_pred["task_id"] = df["task_id"].values
        else:
            print(f"âŒ è¡Œæ•°ä¸åŒ¹é… (Pred: {len(df_pred)} vs Src: {len(df)})ï¼Œå°è¯•é€šè¿‡ merge æ¢å¤...")
            # å¦‚æœçœŸçš„è¡Œæ•°ä¸å¯¹ï¼Œè¿™é‡Œéœ€è¦æ›´å¤æ‚çš„ mergeï¼Œä½†é€šå¸¸ sglang ä¿æŒé¡ºåº
            # ç®€å•å¤„ç†ï¼šæŠ¥é”™é€€å‡ºï¼Œé¿å…é”™ä½
            return

        # æ‰¾é¢„æµ‹åˆ—
        pred_col = None
        for col in df_pred.columns:
            if "pred" in str(col).lower() or "output" in str(col).lower():
                pred_col = col
                break
        
        if not pred_col:
            print("âŒ æ‰¾ä¸åˆ°é¢„æµ‹åˆ—")
            return

        samples = []
        for _, row in df_pred.iterrows():
            raw_tid = str(row["task_id"])
            # ç¡®ä¿æ ¼å¼æ˜¯ Mbpp/123
            tid = raw_tid if raw_tid.startswith("Mbpp/") else f"Mbpp/{raw_tid}"
            
            # è·å–ç”Ÿæˆçš„ä»£ç 
            generated = str(row.get(pred_col, ""))
            
            # æ¸…æ´—
            clean_code = sanitize_code(generated)
            
            samples.append({"task_id": tid, "completion": clean_code})

        with open(SAMPLES_JSONL, "w", encoding="utf-8") as f:
            for s in samples:
                f.write(json.dumps(s, ensure_ascii=False) + "\n")
        print(f"âœ… JSONL ç”Ÿæˆå®Œæ¯•: {SAMPLES_JSONL}")
        
    except Exception as e:
        print(f"âŒ è½¬æ¢ JSONL å¤±è´¥: {e}")
        traceback.print_exc()
        return

    # ---------------- Step 3: è¯„æµ‹ ----------------
    print(f"\n>>> [3/3] è¿è¡Œ EvalPlus...")
    
    # ã€å¼ºåˆ¶åˆ é™¤ç¼“å­˜ã€‘é˜²æ­¢è¯»å–æ—§çš„ 0 åˆ†ç»“æœ
    cache_file = SAMPLES_JSONL.replace(".jsonl", "_eval_results.json")
    if os.path.exists(cache_file):
        os.remove(cache_file)
        print("ğŸ—‘ï¸  å·²åˆ é™¤æ—§çš„è¯„æµ‹ç¼“å­˜æ–‡ä»¶ï¼Œå¼ºåˆ¶é‡æµ‹ã€‚")

    cmd = ["evalplus.evaluate", "--dataset", "mbpp", "--samples", SAMPLES_JSONL]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    try:
        subprocess.run(cmd, check=True)
        print("\nâœ… è¯„æµ‹æµç¨‹ç»“æŸï¼")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ EvalPlus è¿è¡ŒæŠ¥é”™ (Code {e.returncode})")

if __name__ == "__main__":
    main()

