#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æµ‹è¯•å‘½ä»¤è¡Œå·¥å…·
ä½¿ç”¨æ–¹æ³•:
    python auto_test.py --model-path /path/to/model --checkpoint 240 --gpu 1 --port 30064
    python auto_test.py --model-path /path/to/model --gpu 1 --datasets GSM8K MATH500
"""

import subprocess
import time
import os
import sys
import pandas as pd
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

sys.path.append("/data/home/the/rxliu/projects/open-r1-main/tests/utils")
from tqdm import tqdm
from llm_judge import llm_judge_via_api
from inference_sglang import inference_sglang
from openai import OpenAI


class AutoTestManager:
    """è‡ªåŠ¨åŒ–æµ‹è¯•ç®¡ç†å™¨"""
    
    DATASETS = {
        "MATH500": {
            "path": "/ssd5/rxliu/datasets/MATH-500/test.parquet",
            "query_field": "problem",
            "answer_field": "answer",
            "repetitions": 16,
            "max_tokens": 32000,
            "process_gt": lambda x: x
        },
        "GSM8K": {
            "path": "/ssd5/rxliu/datasets/gsm8k/main/test-00000-of-00001.parquet",
            "query_field": "question",
            "answer_field": "answer",
            "repetitions": 3,
            "max_tokens": 30000,
            "process_gt": lambda text: text.split("####")[-1].strip() if isinstance(text, str) and "####" in text else str(text).strip()
        },
        "AIME24": {
            "path": "/ssd5/rxliu/datasets/AIME24/data/train-00000-of-00001.parquet",
            "query_field": "problem",
            "answer_field": "answer",
            "repetitions": 16,
            "max_tokens": 32000,
            "process_gt": lambda x: x
        },
        "AIME25": {
            "path": "/data/home/the/rxliu/projects/dataset_information.py",
            "query_field": "question",
            "answer_field": "answer",
            "repetitions": 16,
            "max_tokens": 32000,
            "process_gt": lambda x: x
        },
        "AMC23": {
            "path": "/ssd5/rxliu/datasets/AMC23/data/test-00000-of-00001.parquet",
            "query_field": "question",
            "answer_field": "answer",
            "repetitions": 16,
            "max_tokens": 32000,
            "process_gt": lambda x: x
        }
    }
    
    def __init__(self, model_path, checkpoint=None, cuda_device="1", port=30064,
                 temperature=0.6, base_output_dir="/data/home/the/rxliu/projects/open-r1-main/tests/results",
                 api_key="sk-8d445207b1ab47efb83069ccc1b845b6",
                 api_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                 judge_model="qwen3-next-80b-a3b-instruct", api_workers=4, conda_env=None):
        
        self.model_path = model_path
        self.checkpoint = checkpoint
        self.cuda_device = cuda_device
        self.port = port
        self.temperature = temperature
        self.base_output_dir = base_output_dir
        self.api_workers = api_workers
        self.conda_env = conda_env
        
        if checkpoint:
            self.full_model_path = os.path.join(model_path, f"checkpoint-{checkpoint}")
            self.model_name = f"{Path(model_path).name}-ckpt{checkpoint}"
        else:
            self.full_model_path = model_path
            self.model_name = Path(model_path).name
        
        self.api_key = api_key
        self.api_url = api_url
        self.judge_model = judge_model
        self.client = OpenAI(api_key=api_key, base_url=api_url)
        self.server_process = None
    
    def cleanup_port(self):
        """æ¸…ç†å¯èƒ½å ç”¨ç«¯å£çš„è¿›ç¨‹"""
        print(f"ğŸ” æ£€æŸ¥ç«¯å£ {self.port} æ˜¯å¦è¢«å ç”¨...")
        try:
            # æŸ¥æ‰¾å ç”¨è¯¥ç«¯å£çš„è¿›ç¨‹
            result = subprocess.run(
                ["lsof", "-ti", f":{self.port}"],
                capture_output=True,
                text=True
            )
            
            if result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                print(f"âš ï¸  å‘ç°ç«¯å£ {self.port} è¢«ä»¥ä¸‹è¿›ç¨‹å ç”¨: {pids}")
                
                for pid in pids:
                    try:
                        print(f"   æ­£åœ¨ç»ˆæ­¢è¿›ç¨‹ {pid}...")
                        subprocess.run(["kill", "-9", pid], check=True)
                        print(f"   âœ… å·²ç»ˆæ­¢è¿›ç¨‹ {pid}")
                    except subprocess.CalledProcessError:
                        print(f"   âš ï¸  æ— æ³•ç»ˆæ­¢è¿›ç¨‹ {pid} (å¯èƒ½éœ€è¦ sudo æƒé™)")
                
                # ç­‰å¾…ç«¯å£é‡Šæ”¾
                time.sleep(2)
                print(f"âœ… ç«¯å£ {self.port} å·²æ¸…ç†\n")
            else:
                print(f"âœ… ç«¯å£ {self.port} æœªè¢«å ç”¨\n")
                
        except FileNotFoundError:
            # lsof å‘½ä»¤ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ fuser
            try:
                result = subprocess.run(
                    ["fuser", "-k", f"{self.port}/tcp"],
                    capture_output=True,
                    text=True
                )
                time.sleep(2)
                print(f"âœ… ç«¯å£ {self.port} å·²æ¸…ç†\n")
            except FileNotFoundError:
                print(f"âš ï¸  æ— æ³•æ£€æŸ¥ç«¯å£å ç”¨ (lsof/fuser å‘½ä»¤ä¸å¯ç”¨)\n")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†ç«¯å£æ—¶å‡ºé”™: {e}\n")
    
    def start_server(self):
        """å¯åŠ¨SGLangæœåŠ¡å™¨"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨SGLangæœåŠ¡å™¨...")
        print(f"   æ¨¡å‹è·¯å¾„: {self.full_model_path}")
        print(f"   ç«¯å£: {self.port}")
        print(f"   GPU: {self.cuda_device}")
        if self.conda_env:
            print(f"   Condaç¯å¢ƒ: {self.conda_env}")
        print(f"{'='*60}\n")
        
        # å…ˆæ¸…ç†ç«¯å£
        self.cleanup_port()
        
        # æ„å»ºå‘½ä»¤
        if self.conda_env:
            # ä½¿ç”¨conda runæ¥åœ¨æŒ‡å®šç¯å¢ƒä¸­è¿è¡Œå‘½ä»¤
            cmd = [
                "conda", "run", "-n", self.conda_env, "--no-capture-output",
                "python", "-m", "sglang.launch_server",
                "--model-path", self.full_model_path,
                "--port", str(self.port),
                "--trust-remote-code"
            ]
        else:
            cmd = [
                "python", "-m", "sglang.launch_server",
                "--model-path", self.full_model_path,
                "--port", str(self.port),
                "--trust-remote-code"
            ]
        
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = str(self.cuda_device)
        
        self.server_process = subprocess.Popen(
            cmd, env=env, stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, text=True
        )
        
        print("â³ ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨...")
        
        # å¥åº·æ£€æŸ¥ï¼šæŒç»­å°è¯•è¿æ¥æœåŠ¡å™¨ï¼Œæœ€å¤šç­‰å¾…5åˆ†é’Ÿ
        import requests
        max_wait_time = 300  # 5åˆ†é’Ÿ
        check_interval = 5   # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
        elapsed_time = 0
        server_ready = False
        
        while elapsed_time < max_wait_time:
            # å…ˆç­‰å¾…ä¸€æ®µæ—¶é—´
            time.sleep(check_interval)
            elapsed_time += check_interval
            
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            if self.server_process.poll() is not None:
                stderr_output = self.server_process.stderr.read() if self.server_process.stderr else ""
                stdout_output = self.server_process.stdout.read() if self.server_process.stdout else ""
                print(f"âŒ æœåŠ¡å™¨è¿›ç¨‹å·²é€€å‡º!")
                if stderr_output:
                    print(f"é”™è¯¯ä¿¡æ¯:\n{stderr_output[:1000]}")
                if stdout_output:
                    print(f"è¾“å‡ºä¿¡æ¯:\n{stdout_output[:1000]}")
                raise RuntimeError("æœåŠ¡å™¨å¯åŠ¨å¤±è´¥!")
            
            # å°è¯•è¿æ¥æœåŠ¡å™¨
            try:
                response = requests.get(f"http://127.0.0.1:{self.port}/health", timeout=2)
                if response.status_code == 200:
                    server_ready = True
                    print(f"âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ! (ç­‰å¾…æ—¶é—´: {elapsed_time}ç§’)\n")
                    break
            except:
                print(f"   ç­‰å¾…ä¸­... ({elapsed_time}ç§’)")
                continue
        
        if not server_ready:
            print(f"âŒ æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶! (ç­‰å¾…äº†{max_wait_time}ç§’)")
            # è¾“å‡ºæœåŠ¡å™¨çš„stderrå’Œstdoutä»¥ä¾¿è°ƒè¯•
            if self.server_process.poll() is None:
                self.server_process.terminate()
                self.server_process.wait(timeout=5)
            stderr_output = self.server_process.stderr.read() if self.server_process.stderr else ""
            stdout_output = self.server_process.stdout.read() if self.server_process.stdout else ""
            if stderr_output:
                print(f"æœåŠ¡å™¨é”™è¯¯ä¿¡æ¯:\n{stderr_output[:1000]}")
            if stdout_output:
                print(f"æœåŠ¡å™¨è¾“å‡ºä¿¡æ¯:\n{stdout_output[:1000]}")
            raise RuntimeError("æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶!")
        
        # é¢å¤–ç­‰å¾…å‡ ç§’ç¡®ä¿å®Œå…¨å°±ç»ª
        print("â³ é¢å¤–ç­‰å¾…10ç§’ç¡®ä¿æœåŠ¡å™¨å®Œå…¨å°±ç»ª...")
        time.sleep(10)
        print("âœ… æœåŠ¡å™¨å·²å®Œå…¨å°±ç»ª!\n")
    
    def stop_server(self):
        """åœæ­¢SGLangæœåŠ¡å™¨"""
        if self.server_process:
            print("\nğŸ›‘ æ­£åœ¨åœæ­¢æœåŠ¡å™¨...")
            
            # å…ˆå°è¯•ä¼˜é›…ç»ˆæ­¢
            self.server_process.terminate()
            try:
                self.server_process.wait(timeout=10)
                print("âœ… æœåŠ¡å™¨å·²åœæ­¢\n")
            except subprocess.TimeoutExpired:
                # å¦‚æœ10ç§’åè¿˜æ²¡åœæ­¢ï¼Œå¼ºåˆ¶æ€æ­»
                print("âš ï¸  æœåŠ¡å™¨æœªå“åº”ï¼Œå¼ºåˆ¶ç»ˆæ­¢...")
                self.server_process.kill()
                self.server_process.wait()
                print("âœ… æœåŠ¡å™¨å·²å¼ºåˆ¶åœæ­¢\n")
            
            # å†æ¬¡æ¸…ç†ç«¯å£ï¼Œç¡®ä¿æ²¡æœ‰é—ç•™è¿›ç¨‹
            time.sleep(2)
            self.cleanup_port()
    
    def process_row(self, row, process_gt_func):
        """å¤„ç†å•è¡Œæ•°æ®"""
        pred = row.get('predicted_answer', '')
        raw_gt = row.get('ground_truth', '')
        clean_gt = process_gt_func(raw_gt)
        is_correct = llm_judge_via_api(pred, clean_gt, self.api_url, self.api_key, self.judge_model)
        
        new_row = row.copy()
        new_row['processed_ground_truth'] = clean_gt
        new_row['is_correct_judge'] = is_correct
        return new_row
    
    def run_single_dataset(self, dataset_name, dataset_config):
        """è¿è¡Œå•ä¸ªæ•°æ®é›†çš„æµ‹è¯•"""
        print(f"\n{'#'*60}")
        print(f"ğŸ“Š å¼€å§‹æµ‹è¯•æ•°æ®é›†: {dataset_name}")
        print(f"{'#'*60}\n")
        
        current_time_str = datetime.now().strftime("%Y%m%d-%H%M%S")
        result_folder = os.path.join(self.base_output_dir, f"{self.model_name}-{dataset_name}-{current_time_str}")
        os.makedirs(result_folder, exist_ok=True)
        print(f"ğŸ“ ç»“æœä¿å­˜è·¯å¾„: {result_folder}\n")
        
        accuracy_list = []
        repetitions = dataset_config["repetitions"]
        
        for i in range(1, repetitions + 1):
            print(f"--- ğŸ§ª ç¬¬ {i}/{repetitions} æ¬¡æµ‹è¯• ---")
            
            inference_output_path = os.path.join(result_folder, f"inference_run_{i}.xlsx")
            result_output_path = os.path.join(result_folder, f"result_run_{i}.xlsx")
            
            try:
                print(f"ğŸ“„ æ‰§è¡Œæ¨ç†...")
                # ç›´æ¥ä¼ é€’ base_url å‚æ•°ï¼Œä¸ä½¿ç”¨ç¯å¢ƒå˜é‡
                inference_sglang(
                    dataset_config["path"], 
                    "", 
                    dataset_config["query_field"],
                    dataset_config["answer_field"], 
                    inference_output_path,
                    self.model_name, 
                    self.temperature, 
                    dataset_config["max_tokens"],
                    base_url=f"http://127.0.0.1:{self.port}/v1"
                )
                    
            except Exception as e:
                print(f"ğŸš¨ æ¨ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            if not os.path.exists(inference_output_path):
                print(f"âŒ æ‰¾ä¸åˆ°æ¨ç†ç»“æœæ–‡ä»¶,è·³è¿‡")
                continue
            
            df = pd.read_excel(inference_output_path)
            data_list = df.to_dict('records')
            
            print(f"âš–ï¸ å¼€å§‹LLM Judgeè¯„æµ‹ ({len(data_list)}æ¡æ•°æ®)")
            with ThreadPoolExecutor(max_workers=self.api_workers) as executor:
                results = list(tqdm(
                    executor.map(lambda row: self.process_row(row, dataset_config["process_gt"]), data_list),
                    total=len(data_list)
                ))
            
            result_df = pd.DataFrame(results)
            num_correct = result_df["is_correct_judge"].sum()
            total = len(result_df)
            accuracy = num_correct / total if total > 0 else 0
            accuracy_list.append(accuracy)
            
            print(f"âœ… æ­£ç¡®: {num_correct}/{total}, å‡†ç¡®ç‡: {accuracy:.4f}\n")
            result_df.to_excel(result_output_path, index=False)
        
        if accuracy_list:
            avg_accuracy = sum(accuracy_list) / len(accuracy_list)
            print(f"\n{'='*60}")
            print(f"ğŸ‰ {dataset_name} å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            print(f"{'='*60}\n")
            
            summary_data = {
                'Run': [f'Run {j+1}' for j in range(len(accuracy_list))] + ['Average'],
                'Accuracy': [f'{acc:.4f}' for acc in accuracy_list] + [f'{avg_accuracy:.4f}']
            }
            summary_df = pd.DataFrame(summary_data)
            summary_output_path = os.path.join(result_folder, "summary_average_accuracy.xlsx")
            summary_df.to_excel(summary_output_path, index=False)
            return avg_accuracy
        
        return None
    
    def run_all_tests(self, datasets=None):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        if datasets is None:
            datasets = list(self.DATASETS.keys())
        
        try:
            self.start_server()
            
            results = {}
            for dataset_name in datasets:
                if dataset_name not in self.DATASETS:
                    print(f"âš ï¸ æœªçŸ¥æ•°æ®é›†: {dataset_name}, è·³è¿‡")
                    continue
                
                avg_acc = self.run_single_dataset(dataset_name, self.DATASETS[dataset_name])
                results[dataset_name] = avg_acc
            
            print("\n" + "="*60)
            print("ğŸ† æ‰€æœ‰æµ‹è¯•å®Œæˆ! æ€»ç»“:")
            print("="*60)
            for dataset, acc in results.items():
                if acc is not None:
                    print(f"  {dataset:15s}: {acc:.4f}")
                else:
                    print(f"  {dataset:15s}: å¤±è´¥")
            print("="*60 + "\n")
            
        finally:
            self.stop_server()


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨åŒ–æ¨¡å‹æµ‹è¯•å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æµ‹è¯•æ‰€æœ‰æ•°æ®é›†(æŒ‡å®šcondaç¯å¢ƒ)
  python auto_test.py --model-path /ssd5/rxliu/models/output/qwen3-8B-Base --checkpoint 240 --gpu 1 --port 30064 --conda-env openr1_rxliu

  # åªæµ‹è¯•ç‰¹å®šæ•°æ®é›†
  python auto_test.py --model-path /ssd5/rxliu/models/output/qwen3-8B-Base --checkpoint 240 --gpu 1 --datasets AIME24 MATH500 --conda-env openr1_rxliu

  # æµ‹è¯•å®Œæ•´æ¨¡å‹(ä¸æŒ‡å®šcheckpoint)
  python auto_test.py --model-path /ssd5/rxliu/models/my-model --gpu 0 --conda-env openr1_rxliu

å¯ç”¨æ•°æ®é›†: MATH500, AIME24, AIME25, AMC23
        """
    )
    
    parser.add_argument('--model-path', required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--checkpoint', type=int, default=None, help='Checkpointå·(å¯é€‰)')
    parser.add_argument('--gpu', default='1', help='GPUè®¾å¤‡å·(é»˜è®¤: 1)')
    parser.add_argument('--port', type=int, default=30064, help='æœåŠ¡å™¨ç«¯å£(é»˜è®¤: 30064)')
    parser.add_argument('--temperature', type=float, default=0.6, help='é‡‡æ ·æ¸©åº¦(é»˜è®¤: 0.6)')
    parser.add_argument('--datasets', nargs='+', default=None, 
                        help='è¦æµ‹è¯•çš„æ•°æ®é›†åˆ—è¡¨(é»˜è®¤: å…¨éƒ¨)')
    parser.add_argument('--output-dir', default='/data/home/the/rxliu/projects/open-r1-main/tests/results',
                        help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--api-workers', type=int, default=4, help='APIå¹¶å‘æ•°(é»˜è®¤: 4)')
    parser.add_argument('--conda-env', default=None, help='Condaç¯å¢ƒåç§°(ä¾‹å¦‚: openr1_rxliu)')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•é…ç½®")
    print("="*60)
    print(f"æ¨¡å‹è·¯å¾„:    {args.model_path}")
    print(f"Checkpoint:  {args.checkpoint if args.checkpoint else 'å®Œæ•´æ¨¡å‹'}")
    print(f"GPUè®¾å¤‡:     {args.gpu}")
    print(f"ç«¯å£:        {args.port}")
    print(f"æ¸©åº¦:        {args.temperature}")
    print(f"æ•°æ®é›†:      {args.datasets if args.datasets else 'å…¨éƒ¨'}")
    print(f"Condaç¯å¢ƒ:   {args.conda_env if args.conda_env else 'å½“å‰ç¯å¢ƒ'}")
    print(f"è¾“å‡ºç›®å½•:    {args.output_dir}")
    print("="*60 + "\n")
    
    manager = AutoTestManager(
        model_path=args.model_path,
        checkpoint=args.checkpoint,
        cuda_device=args.gpu,
        port=args.port,
        temperature=args.temperature,
        base_output_dir=args.output_dir,
        api_workers=args.api_workers,
        conda_env=args.conda_env
    )
    
    manager.run_all_tests(datasets=args.datasets)


if __name__ == "__main__":
    main()