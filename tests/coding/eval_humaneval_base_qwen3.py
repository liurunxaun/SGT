import json
import re
import sys
from pathlib import Path
from typing import Tuple
import packaging.version

from datasets import load_from_disk
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

# ====== 0. ç¯å¢ƒç‰ˆæœ¬æ£€æŸ¥ ======
required_version = "4.37.0" # æ”¾å®½ä¸€ç‚¹é™åˆ¶ï¼Œä½†å»ºè®®è¶Šæ–°è¶Šå¥½
if packaging.version.parse(transformers.__version__) < packaging.version.parse(required_version):
    print(f"âš ï¸  Warning: transformers version {transformers.__version__} is low.")

# ====== é…ç½®åŒº ======

# æ•°æ®é›†è·¯å¾„
HUMANEVALPLUS_DISK_DIR = "/ssd5/rxliu/datasets/humanevalplus"

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/ssd5/rxliu/models/Qwen3-8B" 

# è¾“å‡ºæ–‡ä»¶ 1: è¯„æµ‹ç”¨ (çº¯å‡€ä»£ç )
OUTPUT_JSONL = "/data/home/the/rxliu/projects/open-r1-main/tests/coding/samples_humanevalplus_qwen3_instruct_11.27_2.jsonl"

# è¾“å‡ºæ–‡ä»¶ 2: è°ƒè¯•æ—¥å¿— (Markdown, åŒ…å« Prompt å’Œ æ€è€ƒè¿‡ç¨‹)
OUTPUT_LOG_MD = "/data/home/the/rxliu/projects/open-r1-main/tests/coding/samples_humanevalplus_qwen3_instruct_11.27_2.md"

# å‚æ•°é…ç½®
MAX_NEW_TOKENS = 32768 
TEMPERATURE = 0.6 
TOP_P = 0.95 

# ====== åŠ è½½æ¨¡å‹ ======
print(f"Loading Qwen3 model from {MODEL_PATH} ...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()

def extract_final_code(text: str) -> str:
    """æ¸…æ´—ä»£ç ç”¨äºè¯„æµ‹"""
    # 1. ç§»é™¤ <think> æ ‡ç­¾å†…å®¹
    clean_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
    
    # 2. æå– Python ä»£ç å—
    pattern = re.compile(r'```python\n(.*?)```', re.DOTALL)
    matches = pattern.findall(clean_text)
    
    if matches:
        return matches[-1].strip()
    
    pattern_generic = re.compile(r'```\n(.*?)```', re.DOTALL)
    matches_generic = pattern_generic.findall(clean_text)
    if matches_generic:
        return matches_generic[-1].strip()
        
    return clean_text

def gen_qwen3_solution(humaneval_prompt: str) -> Tuple[str, str, str]:
    """
    è¿”å›ä¸‰ä¸ªå€¼: 
    1. solution (ç”¨äºè¯„æµ‹çš„çº¯ä»£ç )
    2. full_response (æ¨¡å‹ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ï¼Œå« think)
    3. actual_input (å–‚ç»™æ¨¡å‹çš„å®é™… Prompt å­—ç¬¦ä¸²)
    """
    
    content = f"""Please complete the following Python function.
Note: You must wrap the code in ```python ... ``` blocks.

{humaneval_prompt}"""

    messages = [
        {"role": "user", "content": content}
    ]
    
    # å°è¯•åº”ç”¨æ¨¡æ¿
    try:
        text_input = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True,
            # enable_thinking=True # å¦‚æœæŠ¥é”™ä¸æ”¯æŒï¼Œè¯·æ³¨é‡Šæ‰è¿™è¡Œ
        )
    except TypeError:
        text_input = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
    
    inputs = tokenizer(text_input, return_tensors="pt").to(model.device)
    input_len = inputs.input_ids.shape[1]

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    generated_ids = outputs[0][input_len:]
    full_response = tokenizer.decode(generated_ids, skip_special_tokens=True)

    # æå–çº¯ä»£ç 
    clean_code = extract_final_code(full_response)
    
    if "def " in clean_code:
        solution = clean_code
    else:
        solution = humaneval_prompt + "\n" + clean_code

    return solution, full_response, text_input

# ====== ä¸»å‡½æ•° ======

def main():
    print(f"Loading HumanEval+ from {HUMANEVALPLUS_DISK_DIR} ...")
    ds = load_from_disk(HUMANEVALPLUS_DISK_DIR)
    test_set = ds["test"]

    out_path = Path(OUTPUT_JSONL)
    log_path = Path(OUTPUT_LOG_MD)
    
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # æ–­ç‚¹ç»­è·‘
    finished_task_ids = set()
    if out_path.exists():
        print(f"Checking progress in {out_path}...")
        with out_path.open("r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        finished_task_ids.add(data["task_id"])
                    except: pass
        print(f"Found {len(finished_task_ids)} completed samples.")

    open_mode = "a" if len(finished_task_ids) > 0 else "w"
    
    # åŒæ—¶æ‰“å¼€ JSONL å’Œ Markdown æ–‡ä»¶
    with out_path.open(open_mode, encoding="utf-8") as f_json, \
         log_path.open(open_mode, encoding="utf-8") as f_md:
        
        # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™ä¸ª Markdown æ ‡é¢˜
        if open_mode == "w":
            f_md.write("# Qwen3 Testing Log\n\nGenerated outputs with prompts and thinking process.\n\n")

        for i, problem in enumerate(test_set):
            task_id = problem["task_id"]
            if task_id in finished_task_ids:
                continue

            print(f"[{i+1}/{len(test_set)}] Generating for {task_id} ...")
            
            try:
                # è·å– solution (è¯„æµ‹ç”¨), raw_resp (æ—¥å¿—ç”¨), prompt_str (æ—¥å¿—ç”¨)
                solution, raw_resp, prompt_str = gen_qwen3_solution(problem["prompt"])
                
                # 1. å†™å…¥ JSONL
                sample = {"task_id": task_id, "solution": solution}
                f_json.write(json.dumps(sample, ensure_ascii=False) + "\n")
                f_json.flush()
                
                # 2. å†™å…¥ Markdown æ—¥å¿—
                # ä½¿ç”¨ Markdown çš„ä»£ç å—å’Œå¼•ç”¨æ ¼å¼ï¼Œé˜…è¯»ä½“éªŒå¾ˆå¥½
                f_md.write(f"## Task: {task_id}\n\n")
                
                f_md.write(f"### ğŸ“¥ Input Prompt\n")
                f_md.write(f"```text\n{prompt_str}\n```\n\n")
                
                f_md.write(f"### ğŸ“¤ Model Output\n")
                # å¦‚æœæœ‰ think æ ‡ç­¾ï¼ŒMarkdown æ¸²æŸ“é€šå¸¸èƒ½ç›´æ¥æ˜¾ç¤ºï¼Œæˆ–è€…ç”¨å¼•ç”¨å—åŒ…è£¹
                f_md.write(f"> **Raw Response:**\n\n")
                f_md.write(f"{raw_resp}\n\n")
                
                f_md.write(f"### ğŸ Extracted Solution\n")
                f_md.write(f"```python\n{solution}\n```\n")
                
                f_md.write(f"\n---\n\n") # åˆ†éš”çº¿
                f_md.flush()

            except Exception as e:
                print(f"  !! Error: {e}")
                f_md.write(f"## Task: {task_id} - ERROR\n`{str(e)}`\n\n---\n\n")

    print(f"Done! \nJSONL saved to: {out_path}\nMarkdown Log saved to: {log_path}")

if __name__ == "__main__":
    main()